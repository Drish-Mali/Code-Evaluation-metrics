{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a14edcca",
   "metadata": {},
   "source": [
    "## ðŸ§  What is CodeBLEU?\n",
    "\n",
    "**CodeBLEU** (Ren et al., 2020) is an improved metric for evaluating code generated by models (like GPT or CodeT5).  \n",
    "It builds on **BLEU** but adds **code-specific features** to better capture the *semantic* and *syntactic* correctness of programs.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Components of CodeBLEU\n",
    "\n",
    "CodeBLEU combines **four parts**:\n",
    "\n",
    "#### ðŸŸ¦ 1. n-gram Match (like BLEU)\n",
    "- Measures how many tokens overlap with the reference.\n",
    "\n",
    "#### ðŸŸª 2. Syntax Match\n",
    "- Uses the **Abstract Syntax Tree (AST)** of the code to compare structure.  \n",
    "- Example: even if variable names differ, a similar AST means the same code logic.\n",
    "\n",
    "#### ðŸŸ¨ 3. Data Flow Match\n",
    "- Checks whether variables are **defined, modified, and used** in a similar order.  \n",
    "- Captures the **semantic meaning** of the code.\n",
    "\n",
    "#### ðŸŸ© 4. Keyword Matching\n",
    "- Checks the presence of **important language keywords and operators** in the code.  \n",
    "- Ensures that key elements like `return`, `for`, `+`, etc., are correctly used.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Formula (Conceptual)\n",
    "- Combines all the above using weights (default: **0.25 each**).\n",
    "\\[\n",
    "\\text{CodeBLEU} = \\alpha \\times \\text{BLEU} + \\beta \\times \\text{Syntax} + \\gamma \\times \\text{DataFlow} + \\delta \\times \\text{KeywordMatch}\n",
    "\\]\n",
    "\n",
    "(Weights sum to 1)\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ’¡ **In short:**  \n",
    "\n",
    "CodeBLEU extends BLEU by adding structure (syntax) and meaning (semantics) awareness, making it a far better evaluation metric for code generation tasks.\n",
    "\n",
    "### ðŸ”¹ Reference\n",
    "\n",
    "Ren, S., Lu, S., Zhang, R., Zhang, X., Zhang, Y., Wang, X., ... & Yu, P. S. (2020). **CodeBLEU: a Method for Evaluating Code Generation.**  \n",
    "[https://arxiv.org/pdf/2009.10297](https://arxiv.org/pdf/2009.10297)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed51d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'codebleu': 0.5537842627792564, 'ngram_match_score': 0.10419044640136393, 'weighted_ngram_match_score': 0.11094660471566163, 'syntax_match_score': 1.0, 'dataflow_match_score': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from codebleu import calc_codebleu\n",
    "\n",
    "prediction = \"def add ( a , b ) :\\n return a + b\"\n",
    "reference = \"def sum ( first , second ) :\\n return second + first\"\n",
    "\n",
    "result = calc_codebleu([reference], [prediction], lang=\"python\", weights=(0.25, 0.25, 0.25, 0.25))\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b25647d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Prediction 1: {'codebleu': 0.523297991032208, 'ngram_match_score': 0.043472087194499145, 'weighted_ngram_match_score': 0.04971987693433304, 'syntax_match_score': 1.0, 'dataflow_match_score': 1.0}\n",
      "Python Prediction 2: {'codebleu': 0.5219576605651959, 'ngram_match_score': 0.039281465090051315, 'weighted_ngram_match_score': 0.048549177170732344, 'syntax_match_score': 1.0, 'dataflow_match_score': 1.0}\n",
      "Java Prediction 1: {'codebleu': 0.5306039104748079, 'ngram_match_score': 0.05859059370151704, 'weighted_ngram_match_score': 0.06382504819771465, 'syntax_match_score': 1.0, 'dataflow_match_score': 1.0}\n",
      "Java Prediction 2: {'codebleu': 0.5298738234837129, 'ngram_match_score': 0.05637560315259291, 'weighted_ngram_match_score': 0.06311969078225889, 'syntax_match_score': 1.0, 'dataflow_match_score': 1.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from codebleu import calc_codebleu\n",
    "\n",
    "# ==========================\n",
    "# Example 1: Python\n",
    "# ==========================\n",
    "python_ref = \"def sum(first, second):\\n    return second + first\"\n",
    "python_pred1 = \"def add(a, b):\\n    return a + b\"\n",
    "python_pred2 = \"def add(a, b):\\n    return a - b\"\n",
    "\n",
    "result_python1 = calc_codebleu(\n",
    "    [python_ref], [python_pred1],\n",
    "    lang=\"python\",\n",
    "    weights=(0.25, 0.25, 0.25, 0.25),\n",
    "    tokenizer=None  # None uses default Python tokenizer\n",
    ")\n",
    "result_python2 = calc_codebleu(\n",
    "    [python_ref], [python_pred2],\n",
    "    lang=\"python\",\n",
    "    weights=(0.25, 0.25, 0.25, 0.25),\n",
    "    tokenizer=None\n",
    ")\n",
    "\n",
    "print(\"Python Prediction 1:\", result_python1)\n",
    "print(\"Python Prediction 2:\", result_python2)\n",
    "\n",
    "# ==========================\n",
    "# Example 2: Java\n",
    "# ==========================\n",
    "java_ref = \"\"\"\n",
    "public int sum(int first, int second) {\n",
    "    return second + first;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "java_pred1 = \"\"\"\n",
    "public int add(int a, int b) {\n",
    "    return a + b;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "java_pred2 = \"\"\"\n",
    "public int add(int a, int b) {\n",
    "    return a - b;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Use tokenizer=\"none\" to skip Tree-sitter issues\n",
    "result_java1 = calc_codebleu(\n",
    "    [java_ref], [java_pred1],\n",
    "    lang=\"java\",\n",
    "    weights=(0.25, 0.25, 0.25, 0.25),\n",
    "    tokenizer=None\n",
    ")\n",
    "result_java2 = calc_codebleu(\n",
    "    [java_ref], [java_pred2],\n",
    "    lang=\"java\",\n",
    "    weights=(0.25, 0.25, 0.25, 0.25),\n",
    "    tokenizer=None\n",
    ")\n",
    "\n",
    "print(\"Java Prediction 1:\", result_java1)\n",
    "print(\"Java Prediction 2:\", result_java2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec80cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evulation-metrics (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
