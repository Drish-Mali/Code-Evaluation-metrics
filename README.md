# Code Evaluation Metrics

This repository provides tools to evaluate code generated by models, including **CodeBLEU** and other code quality metrics from pyccmetrics. It helps researchers and developers assess the correctness, syntax, and semantic quality of program outputs.

Most of the metric implementations and detailed examples are available in the included Jupyter notebooks.

---

## Installation

1. **Clone the repository**

```bash
git clone https://github.com/Drish-Mali/Code-Evaluation-metrics.git
cd Code-Evaluation-metrics
```
2. **Install uv and create a virtual environment**

```bash
# Create a virtual environment (recommended)
python -m venv .venv

# Activate the virtual environment
# Windows
.venv\Scripts\activate
# macOS / Linux
source .venv/bin/activate
#Install uv in the virtual environment
pip install uv
# Install dependencies using uv
uv sync
#Register the virtual environment as a Jupyter kernel to use it while running notebooks
pip install ipykernel
python -m ipykernel install --user --name=evulation-metrics --display-name "Evulation Metrics"
```

## Metrics Overview

- **CodeBLEU**: Evaluates code generation quality using weighted n-gram matching, syntax, and data-flow similarity.

- **PyCCMetrics**: Computes code complexity metrics such as cyclomatic complexity, maintainability, and code structure metrics.

> Detailed examples and explanations are included in the notebooks.

